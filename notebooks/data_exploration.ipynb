{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Exploration\n",
    "\n",
    "A notebook to investigate the relationships between the different variables, especially the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import ml_colon.data_preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = ml_colon.HERE.parents[2] / \"data\" \n",
    "descr_df = pd.read_csv(data_dir / \"data_description.csv\", index_col=\"column_name\")\n",
    "df = ml_colon.data_preparation.get_df_from_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column: Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.set_title(f\"Histogram of bits\")\n",
    "sns.histplot(df[\"bits\"].values, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(df[\"bits\"], bins=[0, 8, 16, 32, 64, 124], include_lowest=False).value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some block that are encoded with a very small number of bits. 696 rows are encoded with at most 8 bits. \n",
    "\n",
    "Let's identify the characteristics of the rows whose value of \"bits\" is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bits_zero = df.loc[df.bits == 0]\n",
    "print(f\"There are {len(df_bits_zero)} rows with bits = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_descr_df = df_bits_zero.describe()\n",
    "\n",
    "other_cols_zero = list(_descr_df.columns[_descr_df.loc[\"mean\"].eq(0)])\n",
    "\n",
    "print(f\"Other columns that are also 0: {other_cols_zero}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows where the value of `bits` are 0 also have 0 in `bits`, `intra_parts`, `inter_16x16_parts`, `inter_4x4_parts`, `inter_other_parts`, `non_zero_pixels`, `block_movement_h`, `block_movement_v`, `var_movement_h` and `var_movement_v`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumption: Redundant rows\n",
    "When the `bits = 0` we think that it could mean that the information was already encoded in another frame. In other words, they are already represented on the previous frame. To verify the assumption, it is necessary to check if we find duplicated rows in the dataframe disregarding the column `bits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=[c for c in df.columns if c != \"bits\"]).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quick check shows that when considering all columns except `bits` we do not find any duplicated rows. Meaning our assumption was wrong. Furthermore this also means that there are in general no duplicated rows in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are multiple continuous variables in the dataset it is worth to see if there combination of these columns uniquely identifies a row (just out of curiosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=[\"cost_1\", \"cost_2\", \"movement_level\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the combination of the values in the columns `cost_1`, `cost_2` and `movement_level` is already almost unique. Only 2 rows have the exact same values for these 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"cost_1\", \"cost_2\", \"movement_level\"]).filter(lambda x: x[\"bits\"].count() > 1)[[\"bits\", \"cost_1\", \"cost_2\", \"movement_level\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships with target variable\n",
    "In this section we will be looking for high discriminators to determine if a block is relevant. First we produce boxplots for every continuous variable. This could help to see if one group has a different mean/distribution than the other group in the variable relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,25))\n",
    "\n",
    "for num, y in enumerate(df.columns[11:-1]):\n",
    "    ax1 = fig.add_subplot(5,3,num+1)\n",
    "    ax1 = df.boxplot(y, by='relevant', ax=ax1)\n",
    "    \n",
    "plt.suptitle(\"Boxplots of different variables grouped by relevant\", size=20)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be a lot of outliers for the different variables and groups. Some variables seem interesting as the mean of the two groups differs a bit. \n",
    "\n",
    "Now lets take a look at the relationship of the discrete variables with the target variable. For this part, we group by each discrete value and see what percentage in that group has relevant = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(5,5))\n",
    "col_num = [0, 3, 4, 9] # columns quality, skip_parts, inter_16x16_parts and frame_height\n",
    "\n",
    "for ax, y in zip(axes.flatten(), df.columns[col_num]):\n",
    "    df.groupby(y).mean()['relevant'].plot(ax=ax, kind='bar')\n",
    "    \n",
    "plt.suptitle(\"\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.98])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two discrete variables showed many different unique values. Therefore, we handle them as continuous variable in this part of the analysis. We use boxplots to see the different distributions of the data for the groups of relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,3))\n",
    "col_name = ['bits', 'non_zero_pixels']\n",
    "\n",
    "for ax, y in zip(axes.flatten(), col_name):\n",
    "    df.boxplot(y, by='relevant', ax=ax)\n",
    "plt.suptitle(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows 4 different groups starting with a higher peak and followed by two values with smaller counts. These will be grouped together and differences between the percentage of rows where relevant is equal to 1 is shown in the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "proc_df = df[['intra_parts', 'relevant']].copy()\n",
    "proc_df['group'] = proc_df['intra_parts'] // 10 \n",
    "proc_df = proc_df.replace({\"group\": {3: 2, 4:3, 6:4}})\n",
    "proc_df.groupby('group')['relevant'].mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "In this section, the correlations between all variables are investigated. \n",
    "\n",
    "First, in order to consider which variables should be constant and categorical we explore the number of unique values of each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split into continuous and constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = df.loc[:, df.nunique() < 29]\n",
    "cont = df.loc[:, df.nunique() >= 29]\n",
    "varlist = cont.columns.tolist()\n",
    "varlist.append('relevant')\n",
    "contRelv = df[varlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contRelv.groupby('relevant').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this table we can check the difference between the relevant categories, which looks significative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save a copy of the main dataframe for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_test(df, variables, y):\n",
    "    for var in variables:\n",
    "        group0 = df.loc[df[y] == 0][var].tolist()\n",
    "        group1 = df.loc[df[y] == 1][var].tolist()\n",
    "        print(var)\n",
    "        # Some variables have different length depending the group\n",
    "        maxSize = max(len(group0), len(group1))\n",
    "        group0 = random.choices(group0, k = maxSize)\n",
    "        group1 = random.choices(group1, k = maxSize)\n",
    "        \n",
    "        if ttest_rel(group0, group1).pvalue >= 0.5:\n",
    "            print(\"The groups have the same mean.\") \n",
    "        else:\n",
    "            print(\"The groups are different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function we make use of the T-test in order to confirm if the groups are statistically different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_test(contRelv, contRelv.columns[:-1].tolist(), contRelv.columns[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how the categorical variables correlate with the relevant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "corr_mtx = cat.corr()\n",
    "sns.heatmap(corr_mtx, annot = True, cmap = \"YlOrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same with the continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 16))\n",
    "corr_mtx = contRelv.corr()\n",
    "sns.heatmap(corr_mtx, annot = True, cmap = \"YlOrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the number of variables in order to make a more comprenhensive correlation analysis.\n",
    "Some of the variables are highly correlated and their descriptions help to understand how this reductions can me done. After creating new variables we delete the old ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first case we generate a new variable combining frame_height, frame_width and non_zero_pixels in order to obtain precise pixel infromation of the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pixel_frame\"] = df['non_zero_pixels'] / (df['frame_height'] * df['frame_width'])\n",
    "\n",
    "df = df.drop(['frame_height', 'frame_width', 'non_zero_pixels'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sub_mean variable we have put all of them together in order to reduce their correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sub_mean'] = (df['sub_mean_1'] + df['sub_mean_2'] + df['sub_mean_3'] + df['sub_mean_4']) / 4\n",
    "\n",
    "df = df.drop(['sub_mean_1', 'sub_mean_2', 'sub_mean_3', 'sub_mean_4'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sobel variables make reference to the mean of the pixels of the encoded block after applying the Sobel operator in vertical and horizontal direction. So we can combine them easily in order to obtain a mean of both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sobel_hv'] = (df['sobel_h'] + df['sobel_v']) / 2\n",
    "\n",
    "df = df.drop(['sobel_h', 'sobel_v'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variables make reference to the measure of the movement and variance of a certain block in vertical and horizontal. As it is not relevant for this project to keep them separated we can obtain a single variable collecting a mean of this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['movement_var'] = ((df['block_movement_h'] / df['var_movement_h']) + (df['block_movement_v'] / df['var_movement_v'])) / 2\n",
    "\n",
    "df = df.drop(['block_movement_h', 'block_movement_v', 'var_movement_h', 'var_movement_v'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the same combination with the cost variables so we can obtain a better correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cost'] = (df['cost_1'] + df['cost_2']) / 2\n",
    "\n",
    "df = df.drop(['cost_1', 'cost_2'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check if the correlation have been improved with the transformed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 16))\n",
    "corr_mtx = df.corr()\n",
    "sns.heatmap(corr_mtx, annot = True, cmap = \"YlOrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By checking the correlation we can reduce the number of variables but the main method used in this project works under the SelectKBest function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part the analysis is not finished since some variables still having high correlation between them (<0.5). Due that we have decided to drop sobel_hv, sub_mean, variance, bits and inter_other_parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"sobel_hv\", \"sub_mean\", \"variance\", \"bits\", \"inter_other_parts\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the correlation matrix showing the improvement of the results for the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 12))\n",
    "corr_mtx = df.corr()\n",
    "sns.heatmap(corr_mtx, annot = True, cmap = \"YlOrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check if the continuous variables that are still in the analysis are statistically different or if they have the same mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = df.loc[:, df.nunique() >= 29]\n",
    "varlist = cont.columns.tolist()\n",
    "varlist.append('relevant')\n",
    "contRelv = df[varlist]\n",
    "equal_test(contRelv, [\"movement_level\", \"mean\", \"var_sub_blocks\", \"pixel_frame\", \"cost\", \"movement_var\"], contRelv.columns[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use the Label encoder instead of the One Hot Encoding as order could be important for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_code = LabelEncoder()\n",
    "for var in cat.columns:\n",
    "    l_code.fit(cat[var])\n",
    "    cat[var] = l_code.transform(cat[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "In this section, feature selection will be used. This is done with the help of the SelectKBest function of Sklearn.\n",
    "\n",
    "In order to perform the SelectKBest method it is necessary to delete the the Missing Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = dftest[~dftest.sub_mean_3.isnull()]\n",
    "\n",
    "dftest = dftest[~dftest.cost_2.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we create feature and target variable for Classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clf = dftest.loc[:,dftest.columns != 'relevant']\n",
    "\n",
    "y_clf = dftest.loc[:,dftest.columns == 'relevant']\n",
    "\n",
    "X_clf_new = SelectKBest(score_func = chi2, k = 10).fit_transform(X_clf, y_clf)\n",
    "\n",
    "X_clf_new[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the best 10 features that classify the most with the target variable relevant are bits, non_zero_pixels, frame_width, frame_height, movement_level, variance, var_movement_h, var_movement_v, cost_1 and cost_2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
