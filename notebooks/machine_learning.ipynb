{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import tempfile\n",
    "import IPython.display\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.impute\n",
    "import sklearn.ensemble\n",
    "import sklearn.pipeline\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "import sklearn.neural_network\n",
    "import sklearn.preprocessing\n",
    "import sklearn.compose\n",
    "\n",
    "import ml_colon\n",
    "import ml_colon.data_preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Data\n",
    "\n",
    "We have implemented the data cleaning the `ml_colon.data_preparation` module and with that retrieve the \"cleaned\" DataFrame. By \"cleaned\" we mean that we have filtered out all rows that we want to exclude from training. No further rows will be excluded from here onwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ml_colon.data_preparation.get_clean_df_from_csv()\n",
    "\n",
    "print(f\"Loaded data set with {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Train / Test set\n",
    "\n",
    "Next we split the data set into the train / test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2 # 20% of rows\n",
    "features = [c for c in df.columns if c != ml_colon.TARGET_VARIABLE]\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    df[features],\n",
    "    df[ml_colon.TARGET_VARIABLE],\n",
    "    test_size=test_size,\n",
    "    random_state=ml_colon.SEED,\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(y_train)} rows | Test set: {len(y_test)} rows\\n\")\n",
    "print(f\"Train set grouped by relevant: \\n{y_train.value_counts()}\\n\")\n",
    "print(f\"Test set grouped by relevant: \\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling\n",
    "\n",
    "Where are dealing with an imbalanced data set regarding the target variable. Any estimator can reach 82% accuracy by simply always predicting 1.\n",
    "Reminder that:\n",
    "\\begin{equation}\n",
    "accuracy = \\dfrac{TP + TN}{P + N} \n",
    "\\end{equation}\n",
    "\n",
    "Therefore we need to upsample the rows in our dataframe with relevant = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(df: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    \"\"\"Upsample the DataFrame with respect to the target variable \"relevant\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        [description]\n",
    "    n : int\n",
    "        number of rows in resulting DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        the upsampled DataFrame\n",
    "    \"\"\"\n",
    "    weight_relevant_1 = 0.5 / len(df[df.relevant == 1] )\n",
    "    weight_relevant_0 = 0.5 / len(df[df.relevant == 0] )\n",
    "\n",
    "    relevant_weighted = df.relevant.replace({\n",
    "        0: weight_relevant_0,\n",
    "        1: weight_relevant_1\n",
    "    })\n",
    "    return df.sample(\n",
    "        n=n,\n",
    "        replace=True,\n",
    "        weights=relevant_weighted\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = X_train\n",
    "train_df[ml_colon.TARGET_VARIABLE] = y_train\n",
    "\n",
    "train_df = upsample(train_df, n=len(X_train))\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[ml_colon.TARGET_VARIABLE]\n",
    "\n",
    "print(f\"Train set: {len(y_train)} rows\")\n",
    "print(f\"Train set grouped by relevant: \\n{y_train.value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers and Parameters Definition\n",
    "We will try different models to get the best outcome with different hyperparameters based on the accuracy of the model. The different classifiers used are: \n",
    "\n",
    "- K-Nearest Neighbor\n",
    "- Random Forest \n",
    "- Multi-Layer Perceptron\n",
    "\n",
    "With this selection, there is a variety of complexity of models used. There is one simple model, namely the K-Nearest Neighbor algorithm. The Random Forest algorithm is an ensemble method and a more powerful method than K-Nearest Neighbors. Nowadays, there is a big hype around neural networks and their power of finding interesting patterns in data, therefore the Multi-Layer Perceptron is included. \n",
    "\n",
    "Each of these algorithms will be tested with different hyperparameters using grid search. The different values of the parameters are described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_metric = \"accuracy\"\n",
    "assert score_metric in ml_colon.SCORE_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implemented_classifiers = {\n",
    "    \"k_neighbor\": {\n",
    "        \"classifier\": sklearn.neighbors.KNeighborsClassifier(),\n",
    "        \"param_grid\": [{\"classifier__n_neighbors\": [5, 11, 15]}],\n",
    "    },\n",
    "    \"random_forest\": {\n",
    "        \"classifier\": sklearn.ensemble.RandomForestClassifier(max_features=2),\n",
    "        \"param_grid\": [{\"classifier__max_depth\": [4, 8, 10, 12, 14, 16, 18, 20], \"classifier__n_estimators\": [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]}],\n",
    "    },\n",
    "    \"multilayer_perceptron\": {\n",
    "        \"classifier\": sklearn.neural_network.MLPClassifier(),\n",
    "        \"param_grid\": [\n",
    "            {\"classifier__alpha\": [0.001, 0.01, 0.1, 0.5], \"classifier__activation\": [\"identity\", \"relu\"]}\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "classifier = \"k_neighbor\"\n",
    "assert classifier in implemented_classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Pipeline\n",
    "\n",
    "Before a machine learning model can be trained some data transformations need to be done. For that we use a `sklearn.pipeline.Pipeline` to chain the data transformations such as imputing missing values or scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_continous_columns = X_train.columns.get_indexer(\n",
    "    X_train.select_dtypes(include=np.float).columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = sklearn.pipeline.Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"imputer\", sklearn.impute.SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "        ), \n",
    "        (\n",
    "            \"scaler\", sklearn.compose.make_column_transformer(\n",
    "                (\n",
    "                    sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)),\n",
    "                    index_continous_columns\n",
    "                ),\n",
    "                remainder=\"passthrough\"\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            implemented_classifiers[classifier][\"classifier\"],\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_cv = sklearn.model_selection.GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=implemented_classifiers[classifier][\"param_grid\"],\n",
    "    scoring=score_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation \n",
    "\n",
    "We use [mlflow](https://www.mlflow.org/) library to track each model training and save the resulting plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file://\" + str(ml_colon.OUTPUT_DIR / \"mlruns\"))\n",
    "tracking_uri = mlflow.get_tracking_uri()\n",
    "print(\"Current tracking uri: {}\".format(tracking_uri)) # where the outputs are stored\n",
    "\n",
    "mlflow.set_experiment(\"ml_colon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = None\n",
    "experiment_id = None\n",
    "with mlflow.start_run() as run:\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "    experiment_id = run.info.experiment_id\n",
    "    print(f\"Starting MlFlow run {run_id} for experiment {experiment_id}\", \"\\n\")\n",
    "    mlflow.log_param(\"classifier\", classifier)\n",
    "\n",
    "    # The training of the model\n",
    "    print(f\"Training a {classifier} classifier\")\n",
    "    classifier_cv.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameters\n",
    "    print(\"Best parameters:\")\n",
    "    print(classifier_cv.best_params_, \"\\n\")\n",
    "\n",
    "    mlflow.log_params(classifier_cv.best_params_)\n",
    "    mlflow.log_metric(score_metric, classifier_cv.best_score_)\n",
    "\n",
    "    # Evaluation of results\n",
    "    print(\"Classification Report\")\n",
    "    y_pred = classifier_cv.predict(X_test)\n",
    "    y_pred_proba = classifier_cv.predict_proba(X_test)\n",
    "    report = sklearn.metrics.classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    IPython.display.display(pd.DataFrame(report).T)\n",
    "\n",
    "    for label in [\"0.0\", \"1.0\"]:\n",
    "        for k, v in report[label].items():\n",
    "            mlflow.log_metric(f\"label_{label}_{k}\", v)\n",
    "\n",
    "\n",
    "    print(\"Plotting Confusion Matrix\")\n",
    "    cm = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "    cm_fig = sklearn.metrics.ConfusionMatrixDisplay(cm).plot()\n",
    "\n",
    "    print(\"Plotting ROC curve\")\n",
    "    roc_fig = sklearn.metrics.plot_roc_curve(\n",
    "        classifier_cv, X_test, y_test, name=\"ROC Curve\"\n",
    "    )\n",
    "\n",
    "    print(\"Writing Artifacts to MlFlow\")\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        tmp_dir_path = pathlib.Path(tmp_dir)\n",
    "        assert tmp_dir_path.exists()\n",
    "\n",
    "        # Saving plots\n",
    "        cm_fig.figure_.savefig(str(tmp_dir_path / \"confusion_matrix.png\"))\n",
    "        roc_fig.figure_.savefig(str(tmp_dir_path / \"roc.png\"))\n",
    "\n",
    "        # Saving Model\n",
    "        with open(tmp_dir_path / \"model.pkl\", \"wb\") as model_file:\n",
    "            pickle.dump(classifier_cv, model_file)           \n",
    "\n",
    "        mlflow.log_artifacts(str(tmp_dir_path))\n",
    "\n",
    "    # store trained model in list \n",
    "    trained_models.append(classifier_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Compare Different Models\n",
    "\n",
    "Once we have a few models trained we can compare they Receiver Operator Curve (ROC) to see how well perform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "\n",
    "for model in trained_models:\n",
    "    sklearn.metrics.plot_roc_curve(\n",
    "    model, X_test, y_test, name=f\"{model.best_estimator_.steps[-1][1]}\", ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "#### MlFlow Dashboard\n",
    "\n",
    "As explained in the `README.md` you can now run the MlFlow dashboard by executing\n",
    "```\n",
    "cd output/\n",
    "mlflow ui\n",
    "```\n",
    "and then accessing the dashboard through your browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a previously trained model\n",
    "\n",
    "To load a previously trained model all one needs is to specify the run_id in which the model was trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_run_id(experiment_id, run_id):\n",
    "    model_path = ml_colon.OUTPUT_DIR / \"mlruns\" / str(experiment_id) / run_id / \"artifacts\" / \"model.pkl\" \n",
    "    if not model_path.exists():\n",
    "        print(f\"Could not find pickled model under {model_path}\")\n",
    "        raise FileNotFoundError\n",
    "    with open(model_path, \"rb\") as model_file:\n",
    "        return pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reloaded_model = load_model_from_run_id(experiment_id, run_id)\n",
    "\n",
    "print(reloaded_model.estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
