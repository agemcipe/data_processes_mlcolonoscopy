{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import tempfile\n",
    "import IPython.display\n",
    "import pathlib\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.impute\n",
    "import sklearn.ensemble\n",
    "import sklearn.pipeline\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "import sklearn.neural_network\n",
    "import sklearn.preprocessing\n",
    "import sklearn.compose\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import ml_colon\n",
    "import ml_colon.data_preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Data\n",
    "\n",
    "We have implemented the data cleaning the `ml_colon.data_preparation` module and with that retrieve the \"cleaned\" DataFrame. By \"cleaned\" we mean that we have filtered out all rows that we want to exclude from training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ml_colon.data_preparation.get_clean_df_from_csv()\n",
    "\n",
    "print(f\"Loaded data set with {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Train / Test set\n",
    "\n",
    "Next we split the data set into the train / test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2 # 20% of rows\n",
    "possible_features = [c for c in df.columns if c != ml_colon.TARGET_VARIABLE]\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    df[possible_features],\n",
    "    df[ml_colon.TARGET_VARIABLE],\n",
    "    test_size=test_size,\n",
    "    random_state=ml_colon.SEED,\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(y_train)} rows | Test set: {len(y_test)} rows\\n\")\n",
    "print(f\"Train set grouped by relevant: \\n{y_train.value_counts()}\\n\")\n",
    "print(f\"Test set grouped by relevant: \\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling\n",
    "\n",
    "Where are dealing with an imbalanced data set regarding the target variable. Any estimator can reach 82% accuracy by simply always predicting 1.\n",
    "Reminder that:\n",
    "\\begin{equation}\n",
    "accuracy = \\dfrac{TP + TN}{P + N} \n",
    "\\end{equation}\n",
    "\n",
    "Therefore we need to upsample the rows in our dataframe with relevant = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(df: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    \"\"\"Upsample the DataFrame with respect to the target variable \"relevant\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        [description]\n",
    "    n : int\n",
    "        number of rows in resulting DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        the upsampled DataFrame\n",
    "    \"\"\"\n",
    "    weight_relevant_1 = 0.5 / len(df[df.relevant == 1] )\n",
    "    weight_relevant_0 = 0.5 / len(df[df.relevant == 0] )\n",
    "\n",
    "    relevant_weighted = df.relevant.replace({\n",
    "        0: weight_relevant_0,\n",
    "        1: weight_relevant_1\n",
    "    })\n",
    "    return df.sample(\n",
    "        n=n,\n",
    "        replace=True,\n",
    "        weights=relevant_weighted\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = X_train\n",
    "train_df[ml_colon.TARGET_VARIABLE] = y_train\n",
    "\n",
    "train_df = upsample(train_df, n=len(X_train))\n",
    "\n",
    "X_train = train_df[possible_features]\n",
    "y_train = train_df[ml_colon.TARGET_VARIABLE]\n",
    "\n",
    "print(f\"Train set: {len(y_train)} rows\")\n",
    "print(f\"Train set grouped by relevant: \\n{y_train.value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "To speed up the model training and for the beginning have \"simpler\" and potentially more explainable models we limit the numbers of features / columns we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = SelectKBest(score_func = chi2, k = 10).fit(X_train, y_train).get_support()\n",
    "features = [i for (i, boolean) in zip(possible_features, selection) if boolean]\n",
    "\n",
    "X_train_limited_features = X_train[features].copy()\n",
    "X_test_limited_features = X_test[features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Columns to be used as features: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already described in the `data_exploration.ipynb` notebook these are the 10 feature that will hopefully yield the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers and Parameters Definition\n",
    "We will try different models to get the best outcome with different hyperparameters based on the accuracy of the model. The different classifiers used are: \n",
    "\n",
    "- K-Nearest Neighbor\n",
    "- Random Forest \n",
    "- Multi-Layer Perceptron\n",
    "- Gradient Boosting\n",
    "\n",
    "With this selection, there is a variety of complexity of models used. There is one simple model, namely the K-Nearest Neighbor algorithm. The Random Forest algorithm is an ensemble method and a more powerful method than K-Nearest Neighbors. Nowadays, there is a big hype around neural networks and their power of finding interesting patterns in data, therefore the Multi-Layer Perceptron is included. Lastly, we used also a gradient boosting classifier which is as well an ensemble method and similar to Random Forest.\n",
    "\n",
    "Each of these algorithms will be tested with different hyperparameters using grid search. The different values of the parameters are described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implemented_classifiers = {\n",
    "    \"k_neighbor\": {\n",
    "        \"classifier\": sklearn.neighbors.KNeighborsClassifier(),\n",
    "        \"param_grid\": [{\"classifier__n_neighbors\": [3, 5, 11, 15, 20]}],\n",
    "    },\n",
    "    \"random_forest\": {\n",
    "        \"classifier\": sklearn.ensemble.RandomForestClassifier(max_features=2),\n",
    "        \"param_grid\": [{\"classifier__max_depth\": [4, 8, 10, 12, 14, 16, 18, 20], \"classifier__n_estimators\": [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]}],\n",
    "    },\n",
    "    \"multilayer_perceptron\": {\n",
    "        \"classifier\": sklearn.neural_network.MLPClassifier(),\n",
    "        \"param_grid\": [\n",
    "            {\"classifier__alpha\": [0.001, 0.01, 0.1, 0.5], \"classifier__activation\": [\"identity\", \"relu\"]}\n",
    "        ],\n",
    "    },\n",
    "    \"gradient_boosting\": {\n",
    "        \"classifier\": sklearn.ensemble.GradientBoostingClassifier(),\n",
    "        \"param_grid\": [\n",
    "            {\"classifier__learning_rate\": [ 0.05, 0.1, 0.2], \"classifier__n_estimators\": [100, 150, 200]}\n",
    "        ]\n",
    "\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Pipeline\n",
    "\n",
    "Before a machine learning model can be trained some data transformations need to be done. For that we use a `sklearn.pipeline.Pipeline` to chain the data transformations such as imputing missing values or scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pipeline(classifier: str, X_train: pd.DataFrame) -> sklearn.pipeline.Pipeline: \n",
    "    index_continous_columns = X_train.columns.get_indexer(\n",
    "        X_train.select_dtypes(include=np.float).columns\n",
    "    )\n",
    "\n",
    "    pipeline = sklearn.pipeline.Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"imputer\", sklearn.impute.SimpleImputer(missing_values=np.nan, strategy=\"mean\") # this is not strictly necessary as we throw out all rows with missing values in the data cleaning\n",
    "            ), \n",
    "            (\n",
    "                \"scaler\", sklearn.compose.make_column_transformer(\n",
    "                    (\n",
    "                        sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)),\n",
    "                        index_continous_columns\n",
    "                    ),\n",
    "                    remainder=\"passthrough\"\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                implemented_classifiers[classifier][\"classifier\"],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_grid_search(pipeline: sklearn.pipeline.Pipeline) -> sklearn.model_selection.GridSearchCV:\n",
    "    classifier_cv = sklearn.model_selection.GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=implemented_classifiers[classifier][\"param_grid\"],\n",
    "        scoring=score_metric,\n",
    "    )\n",
    "    return classifier_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation \n",
    "\n",
    "We use [mlflow](https://www.mlflow.org/) library to track each model training and save the resulting plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file://\" + str(ml_colon.OUTPUT_DIR / \"mlruns\"))\n",
    "tracking_uri = mlflow.get_tracking_uri()\n",
    "print(\"Current tracking uri: {}\".format(tracking_uri)) # where the outputs are stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(\n",
    "    classifier:str,\n",
    "    X_train: pd.DataFrame, \n",
    "    y_train: pd.Series, \n",
    "    X_test: pd.DataFrame, \n",
    "    y_test: pd.Series\n",
    "    ):\n",
    "    start_time = time.time()\n",
    "\n",
    "    pipeline = build_model_pipeline(classifier, X_train)\n",
    "    classifier_cv = build_grid_search(pipeline)\n",
    "\n",
    "    run_id = None\n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        run_id = run.info.run_id\n",
    "        experiment_id = run.info.experiment_id\n",
    "        print(f\"Starting MlFlow run {run_id} for experiment {experiment_id}\", \"\\n\")\n",
    "\n",
    "        mlflow.log_param(\"classifier\", classifier)\n",
    "        mlflow.log_param(\"number_of_features\", len(X_train.columns))\n",
    "        mlflow.log_param(\"number_of_rows_train\", len(X_train))\n",
    "        mlflow.log_param(\"number_of_rows_test\", len(X_test))\n",
    "\n",
    "        # The training of the model\n",
    "        print(f\"Training a {classifier} classifier\")\n",
    "        classifier_cv.fit(X_train, y_train)\n",
    "\n",
    "        # Best parameters\n",
    "        print(\"Best parameters:\")\n",
    "        print(classifier_cv.best_params_, \"\\n\")\n",
    "\n",
    "        mlflow.log_params(classifier_cv.best_params_)\n",
    "        mlflow.log_metric(score_metric, classifier_cv.best_score_)\n",
    "\n",
    "        # Evaluation of results\n",
    "        print(\"Classification Report\")\n",
    "        y_pred = classifier_cv.predict(X_test)\n",
    "        y_pred_proba = classifier_cv.predict_proba(X_test)\n",
    "        report = sklearn.metrics.classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "        IPython.display.display(pd.DataFrame(report).T)\n",
    "\n",
    "        for label in [\"0.0\", \"1.0\"]:\n",
    "            for k, v in report[label].items():\n",
    "                mlflow.log_metric(f\"label_{label}_{k}\", v)\n",
    "\n",
    "\n",
    "        print(\"Plotting Confusion Matrix\")\n",
    "        cm = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "        cm_fig = sklearn.metrics.ConfusionMatrixDisplay(cm).plot()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Plotting ROC curve\")\n",
    "        roc_fig = sklearn.metrics.plot_roc_curve(\n",
    "            classifier_cv, X_test, y_test, name=f\"ROC Curve {classifier}\"\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Writing Artifacts to MlFlow\")\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            tmp_dir_path = pathlib.Path(tmp_dir)\n",
    "            assert tmp_dir_path.exists()\n",
    "\n",
    "            # Saving plots\n",
    "            cm_fig.figure_.savefig(str(tmp_dir_path / \"confusion_matrix.png\"))\n",
    "            roc_fig.figure_.savefig(str(tmp_dir_path / \"roc.png\"))\n",
    "\n",
    "            # Saving Model\n",
    "            with open(tmp_dir_path / \"model.pkl\", \"wb\") as model_file:\n",
    "                pickle.dump(classifier_cv, model_file)           \n",
    "\n",
    "            mlflow.log_artifacts(str(tmp_dir_path))\n",
    "\n",
    "        # store trained model in list for later evaluation\n",
    "        trained_models.append({\n",
    "            \"name\": classifier,\n",
    "            \"classifier\": classifier_cv,\n",
    "            \"features\": list(X_train.columns),\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test\n",
    "            })\n",
    "\n",
    "    run_time = time.time() - start_time\n",
    "    mlflow.log_metric(\"run_time_in_seconds\", run_time)\n",
    "    print(f\"Finished in {run_time} seconds\")\n",
    "\n",
    "    return run_id, experiment_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's time for training!\n",
    "\n",
    "We want to train each classifier. One time with the preselected features and one time with all columns as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_metric = \"accuracy\"\n",
    "assert score_metric in ml_colon.SCORE_METRICS\n",
    "\n",
    "run_id = None\n",
    "mlflow.set_experiment(\"ml_colon_limited_features\")\n",
    "for classifier, _ in implemented_classifiers.items():\n",
    "    run_id, experiment_id = run_model(classifier, X_train_limited_features, y_train, X_test_limited_features, y_test)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = None\n",
    "mlflow.set_experiment(\"ml_colon_all_features\")\n",
    "for classifier, _ in implemented_classifiers.items():\n",
    "    run_id, experiment_id = run_model(classifier, X_train, y_train, X_test, y_test)\n",
    "    print(\"-------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Compare Different Models\n",
    "\n",
    "Once we have a few models trained we can compare their Receiver Operator Curve (ROC) to see how well perform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "\n",
    "for model in trained_models:\n",
    "\n",
    "    sklearn.metrics.plot_roc_curve(\n",
    "    model['classifier'], model['X_test'], model['y_test'], name=f\"{model['classifier'].best_estimator_.steps[-1][1]}_#features_{len(model['features'])}\", ax=ax\n",
    "    )\n",
    "    \n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "#### MlFlow Dashboard\n",
    "\n",
    "As explained in the `README.md` you can now run the MlFlow dashboard by executing\n",
    "```\n",
    "cd output/\n",
    "mlflow ui\n",
    "```\n",
    "and then accessing the dashboard through your browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a previously trained model\n",
    "\n",
    "To load a previously trained model all one needs is to specify the run_id in which the model was trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_run_id(experiment_id, run_id):\n",
    "    model_path = ml_colon.OUTPUT_DIR / \"mlruns\" / str(experiment_id) / run_id / \"artifacts\" / \"model.pkl\" \n",
    "    if not model_path.exists():\n",
    "        print(f\"Could not find pickled model under {model_path}\")\n",
    "        raise FileNotFoundError\n",
    "    with open(model_path, \"rb\") as model_file:\n",
    "        return pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reloaded_model = load_model_from_run_id(experiment_id, run_id)\n",
    "\n",
    "print(reloaded_model.estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
